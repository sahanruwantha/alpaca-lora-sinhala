# Sinhala Language Fine-tuned Alpaca 7b Model

## Overview
This repository contains the code and resources for fine-tuning the Alpaca 7b model for the Sinhala language. The Alpaca 7b model is a powerful language model, and this project aims to adapt it to the Sinhala language through fine-tuning.

## Dataset
The Alpaca dataset has been translated into Sinhala using Google Translator. Please note that machine translations may not be perfect, and manual verification and correction of translations are recommended for optimal performance. You can find the dataset [https://huggingface.co/datasets/yahma/alpaca-cleaned](www.huggingface.com).

### Translated Dataset
The translated dataset is available in the `data` directory. It includes both the source (original Alpaca) and target (Sinhala translated) texts. You can find the dataset [https://huggingface.co/datasets/sahanruwantha/alpaca-sinhala](www.huggingface.com).

## Pre-trained Model
If you don't want to go through the fine-tuning process, a pre-trained Sinhala model is available in the `pretrained_model` directory. You can use this model for Sinhala language tasks directly. Check out the pre-trained model [here](www.huggingface.com).

## License
This project is licensed under the [MIT License](LICENSE.md).

## Acknowledgments
- The Alpaca 7b model and dataset by the original authors
- Google Translator for initial dataset translation.

Feel free to contribute to this project and improve the Sinhala language model.

Happy fine-tuning!
