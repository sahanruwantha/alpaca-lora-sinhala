# Sinhala Language Fine-tuned Alpaca 7b Model

## Overview
This repository contains the code and resources for fine-tuning the Alpaca 2b model for the Sinhala language. The Alpaca 2b model is a powerful language model, and this project aims to adapt it to the Sinhala language through fine-tuning.

## Dataset
The Alpaca dataset has been translated into Sinhala using Google Translator. Please note that machine translations may not be perfect, and manual verification and correction of translations are recommended for optimal performance.

### Translated Dataset
The translated dataset is available in the `data` directory. It includes both the source (original Alpaca) and target (Sinhala translated) texts.

## Fine-tuning
The `fine-tuning` directory contains scripts and configuration files for fine-tuning the Alpaca 2b model on the Sinhala dataset. Follow the instructions in the `fine-tune.md` file to replicate the fine-tuning process.

## Pre-trained Model
If you don't want to go through the fine-tuning process, a pre-trained Sinhala model is available in the `pretrained_model` directory. You can use this model for Sinhala language tasks directly.

## Evaluation
For evaluating the performance of the fine-tuned model, check the `evaluation` directory. It includes scripts and metrics to assess the model's effectiveness on Sinhala language tasks.

## Usage
Instructions on how to use the fine-tuned Sinhala model in your applications are detailed in the `usage.md` file.

## License
This project is licensed under [MIT License](LICENSE.md).

## Acknowledgments
- The Alpaca model and dataset by the original authors. (Provide proper attribution)
- Google Translator for initial dataset translation.

Feel free to contribute to this project and improve the Sinhala language model.

Happy fine-tuning!
